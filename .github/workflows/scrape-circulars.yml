name: Scrape DTE Circulars

on:
  schedule:
    # Run every 30 minutes
    - cron: '*/30 * * * *'
  workflow_dispatch: # Allow manual trigger
  push:
    branches: [ main ]

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 18  # Increased timeout for 4 data sources
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4 urllib3

    - name: Run scraper
      timeout-minutes: 15  # Increased timeout for 4 data sources
      continue-on-error: false  # Ensure we catch failures
      run: |
        python scraper.py

    - name: Check for changes
      id: verify-changed-files
      run: |
        if [ -n "$(git status --porcelain)" ]; then
          echo "changed=true" >> $GITHUB_OUTPUT
        else
          echo "changed=false" >> $GITHUB_OUTPUT
        fi

    - name: Commit and push changes
      if: steps.verify-changed-files.outputs.changed == 'true'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add circulars.json
        git commit -m "Update circulars data - $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
        git push

    - name: Deploy to GitHub Pages
      if: steps.verify-changed-files.outputs.changed == 'true'
      uses: peaceiris/actions-gh-pages@v3
      with:
        github_token: ${{ secrets.GITHUB_TOKEN }}
        publish_dir: ./
        exclude_assets: '.github,scraper.py,requirements.txt,README.md'